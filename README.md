# Сервис автоматической суммаризации новостных текстов

Проект по созданию DL-модели и сервиса для генерации коротких, осмысленных выжимок (саммари) из длинных новостных статей на русском языке.

## Цель проекта

Основная цель — **повысить продуктивность работы с информацией**, предоставляя пользователям сервис для быстрого получения сути из больших текстов. Это позволяет экономить время аналитиков, редакторов и менеджеров, ускоряя принятие решений и мониторинг информационного поля.

## Набор данных

Для создания и всесторонней оценки модели будут использоваться несколько публичных датасетов, что позволит проверить её устойчивость и способность к обобщению.

1.  **[Gazeta](https://huggingface.co/datasets/IlyaGusev/gazeta)**
    - **Описание:** Содержит ~60,000 пар "статья-саммари" с российских новостных сайтов. Будет использоваться как основной источник данных для обучения (fine-tuning) модели.

2.  **[MLSUM](https://huggingface.co/datasets/reciTAL/mlsum)**
    - **Описание:** Большой многоязычный датасет для суммаризации. (Будет использоваться для экспериментов с дообучением русскоязычная часть)
   
3.  **[РИА Новости](https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta)**
    - **Описание:** Корпус новостных статей от агентства "РИА Новости".
    Будет использоваться как **независимый тестовый набор**. Модель, обученная на `Gazeta` и `MLSUM`, будет оцениваться на этих данных, чтобы проверить её способность работать с текстами другого стиля и источника.

## План экспериментов

1.  **Исследовательский анализ данных (EDA):**
    - Проанализировать распределение длин исходных текстов и саммари. (для дальнейшей настройки обучения)
    - Построить облака слов для текстов и саммари, чтобы понять основные темы. 
    - Проверить данные на наличие шумов, пустых строк и аномалий.

2.  **Выбор Baseline-модели:**
    - Реализовать простой  метод, например, `TextRank` или `LexRank`. Это даст baseline, с которым будет сравниваться DL-модель.

3.  **Выбор и подготовка основной DL-модели:**
    - В качестве основной модели будет использоваться предобученная seq2seq. Основной кандидат — **`google/mt5-small`**.

4.  **Обучение (Fine-tuning):**
    - Написать скрипт для fine-tuning'а модели
    - В качестве основной метрики для валидации в процессе обучения использовать **ROUGE Score**.

5.  **Оценка:**
    - Оценить финальную модель на отложенной тестовой выборке по метрикам `ROUGE-1`, `ROUGE-2` и `ROUGE-L`.
    - Провести качественный анализ: вручную прочитать 50-100 сгенерированных саммари и сравнить их с эталонными. Оценить связность, точность и отсутствие "галлюцинаций".
    - Попробовать разные стратегии декодирования (beam search, top-k sampling) для улучшения качества генерации.


## Целевые метрики 

- Среднее время ответа модели: ≤ 3 секунды на статью до 1000 слов.
- Доля неуспешных запросов: ≤ 1 %.

Использование ресурсов (SLA):

- GPU-память (VRAM): < 8 ГБ на инстанс.
- RAM: < 16 ГБ на инстанс.

Качество модели (ключевая метрика для суммаризации):

- ROUGE-L: ≥ 0.3
- Качественная оценка: Доля сгенерированных саммари, которые являются связными и не содержат "галлюцинаций", должна быть ≥ 90% по результатам ручной проверки на 100 примерах.


## Запуск обучения
`pip install -r requirements.txt`

Собираем конфиг обучения пример в configs/train_config.yaml
Запуск обучения 

`python src/train.py configs/train_config.yaml`

Результаты в папке с результатами из конфига 

Запуск валидации

С базовой моделью (без дообучения)

`python run_evaluation.py --config_path ../configs/train_config.yaml`

С дообученной моделью

`python run_evaluation.py --config_path ../configs/train_config.yaml --model_path $PATH_TO_CKPT`

На выходе метрики и примеры генераций

## Воспроизведение результатов

Загрузка данных и моделей

`dvc pull`

Эта команда свяжется с удаленным хранилищем на Google Drive (в .dvc/config есть аддрес)

Для запуска всего пайплайна (подготовка данных, обучение, оценка) с нуля на основе текущего кода и данных:

`dvc repro`

## Трекинг экспериментов с MLflow

Этот проект использует [MLflow](https://mlflow.org/) для логирования и отслеживания экспериментов.

Для того чтобы отслеживать ваши эксперименты 

1.  **Убедитесь, что вы находитесь в корне проекта** и ваше виртуальное окружение активировано (установлены библиотеки из requirements)

2.  **Запустите MLflow UI:**
`mlflow ui`

3.  **Откройте в браузере** адрес, который появится в терминале (обычно `http://127.0.0.1:5000`).

В веб-интерфейсе вы сможете просматривать все запуски обучения, сравнивать их параметры (learning rate, количество эпох), метрики (ROUGE, loss) и просматривать сохраненные артефакты.

## Запуск через Docker

Проект упакован в Docker-образ для легкого и воспроизводимого запуска офлайн-инференса.

### 1. Сборка образа

Перед сборкой убедитесь, что у вас локально есть обученная модель (выполните `dvc pull`, если необходимо). Модель должна находиться в папке `results/`.

`docker build -t ml-summarizer:v1 .`

### 2. Подготовка данных

Скрипт внутри контейнера ожидает на вход CSV-файл с одной колонкой под названием text.

Создайте папку input и положите в нее ваш файл, например, data.csv. Также создайте пустую папку output для результатов.

### 3. Запуск инференса

`docker run --rm \
  -v $(pwd)/input:/app/input \
  -v $(pwd)/output:/app/output \
  ml-summarizer:v1 \
  --input_path /app/input/data.csv \
  --output_path /app/output/predictions.csv`

После выполнения команды в вашей локальной папке output появится файл predictions.csv с оригинальными текстами и их саммари


## Развертывание как онлайн-сервис (TorchServe)

Модель упакована для развертывания в продакшене как REST API сервис с помощью TorchServe.

### 1. Подготовка архива модели (.mar)

`CHECKPOINT_DIR=./results__

torch-model-archiver \
  --model-name summarizer \
  --version 1.0 \
  --serialized-file $CHECKPOINT_DIR/model.safetensors \
  --handler ./torchserve/summarizer_handler.py \
  --extra-files "$CHECKPOINT_DIR/config.json,$CHECKPOINT_DIR/tokenizer.json,$CHECKPOINT_DIR/spiece.model,$CHECKPOINT_DIR/tokenizer_config.json,$CHECKPOINT_DIR/special_tokens_map.json" \
  --export-path ./model_store \
  --force`

### 2. Сборка и запуск Docker-контейнера

`docker build -f Dockerfile.torchserve -t summarizer-serve:v1 .`
`docker run --rm -d -p 8080:8080 -p 8081:8081 --name my-summarizer-container summarizer-serve:v1`

### 3. Отправка запросов

Проверка загруженных моделей (нужно дождаться пока модель загрузится):

`curl http://localhost:8081/models`

Ожидаемый ответ: JSON с вашей моделью summarizer.

Формат входных данных: JSON с ключом text.

`{
  "text": "Ваш длинный текст для суммаризации находится здесь..."
}`

Запрос на суммаризацию:
`curl -X POST http://localhost:8080/predictions/summarizer -T sample_input.json`